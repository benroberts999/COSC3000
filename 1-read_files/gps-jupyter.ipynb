{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot GPS atomic clock data\n",
    "\n",
    "This file downloads and plots data from the atomic clocks onboard the GPS satellites for a given day.\n",
    "\n",
    "This dats is publicly available at: sideshow.jpl.nasa.gov/pub/jpligsac/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPS data is stored in files by week number (since 1970) and day (of the week)\n",
    "\n",
    "Download it (if we don't have it already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week = 2299\n",
    "day_of_week = 0\n",
    "\n",
    "url = \"https://sideshow.jpl.nasa.gov/pub/jpligsac/\" + str(week) + \"/\"\n",
    "filename = \"jpl\" + str(week) + str(day_of_week) + \".clk.gz\"\n",
    "\n",
    "# Download the file (if we don't have it already)\n",
    "if not path.isfile(filename):\n",
    "        downloaded_file, info = request.urlretrieve(\n",
    "            url + filename,\n",
    "            filename,\n",
    "        )\n",
    "        assert downloaded_file == filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File format -- header\n",
    "\n",
    "This file has a large 'header'.\n",
    "It's usually ~96 lines long, but not always.\n",
    "The header ends with the string: \"END OF HEADER\"\n",
    "There is some useful info up there, but we don't need it.\n",
    "We find the '\"END OF HEADER\"' string, and read the file from there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_line_containing(file, the_string) -> int:\n",
    "    for num, line in enumerate(file, 1):\n",
    "        if the_string in line:\n",
    "            return num\n",
    "\n",
    "# Unip the file, and \"read\" as \"text\" to search for the string\n",
    "unziped = gzip.open(filename, \"rt\")\n",
    "header_line = find_line_containing(unziped, \"END OF HEADER\")\n",
    "print(\"Found\", header_line, \"header lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pandas to read the data file\n",
    "\n",
    "It is white-space delimetered (sometimes with multiple spaces).\n",
    "\n",
    "The file is in a zipped (.gz) format - python knows how to deal with this.\n",
    "\n",
    "It has 11 columns.\n",
    "\n",
    " * There are two \"types\" of clocks - satellite clocks (labelled \"AS\"), and ground recievers (\"AR\")\n",
    " * Each satelite clock has a name \"G01\" - \"G32\"\n",
    " * The main data is the time-stamp (YY MM DD HH MM SS), and the \"bias\"\n",
    " * The bias is the clock time difference (bias) compared to a common station clock\n",
    " * The \"error\" listed is actually just a fraction of the true error due to data processing\n",
    "\n",
    "Here, we use pandas, because of its ability to deal with complicated data containing multiple types (in this case: strings, integers, and floats).\n",
    "\n",
    "You could achieve the same results just with numpy, but this is more neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    filename,\n",
    "    skiprows=header_line,\n",
    "    delimiter=\" \",\n",
    "    skipinitialspace=True,  # Interpret multiple spaces as 1 delimiter\n",
    "    names=(  # Optional: give meaningful names to each column of data file:\n",
    "        \"Type\",\n",
    "        \"Clock\",\n",
    "        \"Year\",\n",
    "        \"Month\",\n",
    "        \"Day\",\n",
    "        \"Hour\",\n",
    "        \"Minute\",\n",
    "        \"Second\",\n",
    "        \"ncols\",\n",
    "        \"Bias\",\n",
    "        \"Error\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some useful info:\n",
    " * separate the \"reciever\" from \"satellite\" clocks\n",
    " * Extract the date\n",
    " * List unique satellite clock names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat_data = data[data.Type == \"AS\"]\n",
    "\n",
    "# Get the date from the file (it is in the filename, but here we get it from the file)\n",
    "date_string = (\n",
    "    str(sat_data.iloc[0][\"Year\"])\n",
    "    + \"-\"\n",
    "    + str(sat_data.iloc[0][\"Month\"])\n",
    "    + \"-\"\n",
    "    + str(sat_data.iloc[0][\"Day\"])\n",
    ")\n",
    "\n",
    "print(\"Date of file:\", date_string)\n",
    "print()\n",
    "\n",
    "# Generate the list of unique satellite clocks:\n",
    "clock_list = sat_data[\"Clock\"].unique()\n",
    "print(\"List of satellite clocks:\")\n",
    "print(clock_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert time-stamps to plottable floats\n",
    "\n",
    "The time stamps are given in an annoying \"yy dd mm hh mm ss\" format - \n",
    "We want to convert to a plottable float.\n",
    "\n",
    "\n",
    "Define a function to convert \"d/h/m/s\" to \"hours since start of day\".\n",
    "\n",
    "We only plot 1 day, so we can ignore year/month.\n",
    "\n",
    "We need to include the day, because midnight appears twice in the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(row):\n",
    "    return (\n",
    "        row[\"Day\"] * 24.0 * 60.0 * 60.0\n",
    "        + row[\"Hour\"] * 60.0 * 60.0\n",
    "        + row[\"Minute\"] * 60.0\n",
    "        + row[\"Second\"]\n",
    "        - sat_data.iloc[0][\"Day\"] * 24.0 * 60.0 * 60.0\n",
    "    ) / (60.0 * 60.0)\n",
    "\n",
    "\n",
    "# Apply this conversion to each row, store in new column \"Time\"\n",
    "sat_data[\"Time\"] = sat_data.apply(convert_time, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "\n",
    "When plotting, we change the y-axis to nanoseconds, for improved legibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When plotting, we change x-axis units to hours, and y-axis to nanoseconds,\n",
    "# for improved legibility\n",
    "plt.title(\"Raw bias data\")\n",
    "plt.xlabel(\"Hours since \" + date_string)\n",
    "plt.ylabel(\"Bias (ns)\")\n",
    "\n",
    "for clock in clock_list:\n",
    "    t_data = sat_data[sat_data.Clock == clock]\n",
    "    plt.plot(t_data[\"Time\"], t_data[\"Bias\"] * 1e9, label=clock)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove large offset (calibration)\n",
    "\n",
    "The clocks all show a reasonably significant offset.\n",
    "This is essentially an \"initial calibration offset\", and can be removed without any issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Subtract mean\")\n",
    "plt.xlabel(\"Hours since \" + date_string)\n",
    "plt.ylabel(\"Bias (ns)\")\n",
    "\n",
    "for clock in clock_list:\n",
    "    t_data = sat_data[sat_data.Clock == clock]\n",
    "    bias = t_data[\"Bias\"]\n",
    "    bias -= np.mean(bias)\n",
    "    plt.plot(t_data[\"Time\"], bias * 1e9, label=clock)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtract polynomial, and add offset\n",
    "\n",
    "All the clocks show a significant drift.\n",
    "\n",
    "Depending on the reason for studying the data, the drift can be modelled and removed.\n",
    "The common method is to subtract a 2nd-order polynomal (fitted for each day).\n",
    "We also add a constant offset to each clock, so we can see them on the plot.\n",
    "Finally, we only plot every second clock, because there's so many.\n",
    "\n",
    "We also plot the clock error bars, using faint dashed black lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Polynomial subtracted + offset + every second + errors\")\n",
    "plt.xlabel(\"Hours since \" + date_string)\n",
    "plt.ylabel(\"Bias (ns)\")\n",
    "\n",
    "\n",
    "def poly(c, x):\n",
    "    return c[0] * x**2 + c[1] * x + c[2]\n",
    "\n",
    "\n",
    "offset = 1e-9  # 1 ns\n",
    "for count, clock in enumerate(clock_list):\n",
    "    if count % 2 == 0:\n",
    "        continue\n",
    "    t_data = sat_data[sat_data.Clock == clock]\n",
    "    bias = t_data[\"Bias\"]\n",
    "    time = t_data[\"Time\"]\n",
    "    c = np.polyfit(time, bias, 2)\n",
    "    bias -= poly(c, time)\n",
    "    bias += count * offset / 2  # (/2) because plot every second\n",
    "    error1 = bias + t_data[\"Error\"]\n",
    "    error2 = bias - t_data[\"Error\"]\n",
    "    plt.plot(time, bias * 1e9, label=clock)\n",
    "    plt.plot(time, error1 * 1e9, \"k--\", linewidth=0.5)\n",
    "    plt.plot(time, error2 * 1e9, \"k--\", linewidth=0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
