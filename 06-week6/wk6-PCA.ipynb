{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy import linalg as LA\n",
    "\n",
    "data = np.genfromtxt(\"assp.csv\", delimiter=\",\")\n",
    "# Just give each column a random name\n",
    "collumns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "print(collumns)\n",
    "\n",
    "num_variables = len(collumns)\n",
    "\n",
    "assert num_variables == len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data\n",
    "* As usual, step 1 is to apply uni/bi-variate methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, num_variables):\n",
    "    plt.plot(data[:, 0], data[:, i], \".\", label=collumns[i])\n",
    "plt.xlabel(collumns[0])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.title(\"Each variable as a function of a\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(ncols=2, width_ratios=[2, 1])\n",
    "ax1.violinplot(data[:, 0:4], showmeans=True)\n",
    "ax1.set_xticks([1, 2, 3, 4], collumns[:4])\n",
    "\n",
    "ax2.violinplot(data[:, 4:], showmeans=True)\n",
    "ax2.set_xticks([1, 2], collumns[4:])\n",
    "\n",
    "plt.suptitle(\"Violin plot, showing means\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a corner plot, with histograms along edges:\n",
    "def corner_plot(data, labels, title):\n",
    "    num_variables = len(labels)\n",
    "    fig, axs = plt.subplots(nrows=num_variables, ncols=num_variables, figsize=(7, 7))\n",
    "    for i in range(num_variables):\n",
    "        for j in range(num_variables):\n",
    "\n",
    "            # Only plot unique lower triangle\n",
    "            if j > i:\n",
    "                axs[i, j].set_visible(False)\n",
    "                continue\n",
    "\n",
    "            # Plot the data\n",
    "\n",
    "            # Scatter plot for when x is not y\n",
    "            if i != j:\n",
    "                axs[i, j].plot(data[:, j], data[:, i], \".\")\n",
    "\n",
    "            # When x=y the plot would just be a straight line\n",
    "            # It's common to plot histgoram (or box plot, etc)\n",
    "            else:\n",
    "                axs[i, j].hist(data[:, i], density=True, alpha=0.4)\n",
    "                x = np.linspace(min(data[:, i]), max(data[:, i]), 100)\n",
    "                kde = stats.gaussian_kde(data[:, i])\n",
    "                axs[i, j].plot(x, kde(x), \"b\")\n",
    "                axs[i, j].fill_between(x, kde(x), alpha=0.6)\n",
    "                axs[i, j].set_yticks([])\n",
    "\n",
    "            # Add title (mean and sem) above diagonal elements:\n",
    "            if i == j:\n",
    "                mean = np.mean(data[:, i])\n",
    "                sem = stats.sem(data[:, i], ddof=1)\n",
    "                axs[i, j].set_title(\n",
    "                    f\"{labels[j]}\\n{mean:.1f}$\\\\pm${sem:.1f}\", fontsize=11\n",
    "                )\n",
    "\n",
    "            # Add x labels only to last row\n",
    "            if i == num_variables - 1:\n",
    "                axs[i, j].set_xlabel(labels[j], fontsize=12)\n",
    "            else:\n",
    "                axs[i, j].set_xticks([])\n",
    "\n",
    "            # Add y labels only to first column\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel(labels[i], fontsize=12)\n",
    "            else:\n",
    "                axs[i, j].set_yticks([])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    fig.align_ylabels(axs[:, 0])\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "fig, axs = corner_plot(data, collumns, \"Corner plot (raw data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the mean of each column:\n",
    "column_means = np.mean(data, axis=0)\n",
    "\n",
    "# Change number of digits printed\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "print(column_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component analysis\n",
    "\n",
    "### 1. Calculate covariance matrix\n",
    "\n",
    "**Reminder**\n",
    "\n",
    "Variance (sample variance):\n",
    "\n",
    "$$\n",
    "  \\sigma^2(X) = \\langle{(X-\\bar X)^2}\\rangle = \\sum_i \\frac{(X-\\bar X)^2}{N-1}\n",
    "$$\n",
    "\n",
    "**Co**-variance\n",
    "\n",
    "$$\n",
    "  {\\rm cov}(X,Y) = \\langle{(X-\\bar X)(Y-\\bar Y)}\\rangle = \\sum_i \\frac{(X-\\bar X)(Y-\\bar Y)}{N-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "  {\\rm cov}(X,X) = \\sigma^2(X)\n",
    "$$\n",
    "\n",
    "\n",
    "**Normalised** covariance (also called correlation coefficent)\n",
    "\n",
    "$$\n",
    "  {\\rm ncov}(X,Y) = \\frac{{\\rm cov}(X,Y)}{\\sqrt{\\sigma^2(X)\\,\\sigma^2(Y)}}\n",
    "$$\n",
    "\n",
    "Note that the normalised covariance is related to the autocorrelation function:\n",
    "$$\n",
    "  {\\rm ncov}\\Big(X(t),\\,X(t+\\Delta t)\\Big) = {\\rm ACF}(\\Delta t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance matrix\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.cov.html\n",
    "# If rowvar is True (default), then each row represents a variable\n",
    "\n",
    "cov = np.cov(data, rowvar=False)\n",
    "\n",
    "print(\"Covariance matrix:\")\n",
    "print(cov)\n",
    "\n",
    "\n",
    "column_variances = np.var(data, axis=0, ddof=1)\n",
    "print(\"\\nVariances of each collumn (a,b,c..):\")\n",
    "print(column_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"normalised\" covariance (or correlation coeficient)\n",
    "# corrcoef = R_ij := C_ij / Sqrt( C_ii*C_jj )\n",
    "ncov = np.corrcoef(data, rowvar=False)\n",
    "\n",
    "print(\"\\nNormalised Covariance matrix:\")\n",
    "print(ncov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find principal components: eigenvalue problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use eigh (instead of eig) since covariance is symmetric\n",
    "evals, evecs = LA.eigh(ncov)\n",
    "\n",
    "# Unsorted e.vals:\n",
    "print(evals)\n",
    "\n",
    "\n",
    "# The eigenvalues are not guarenteed to be in any specific order\n",
    "# It's nice to have them sorted\n",
    "# Must use argsort since we need to sort eigenvectors in same way\n",
    "idx = np.argsort(evals)[::-1]\n",
    "print(\"order:\", idx)\n",
    "\n",
    "# Sort the e.vals and e.vectors:\n",
    "evals = evals[idx]\n",
    "evecs = evecs[:, idx]\n",
    "\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(evals)\n",
    "\n",
    "# Eigenvalues are the relative contibutions\n",
    "contributions = evals / np.sum(evals)\n",
    "\n",
    "print(\"\\nComponent contributions:\")\n",
    "print(contributions)\n",
    "\n",
    "print(\"\\nCumulative component contributions:\")\n",
    "print(np.cumsum(contributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvectors:\")\n",
    "print(evecs)\n",
    "\n",
    "print(\"\\nPrincipal components:\")\n",
    "for n in range(num_variables):\n",
    "    print(\n",
    "        f\"PCA({n}): \",\n",
    "        \"\".join([f\" {evecs[i,n]:+.2f}*{collumns[i]}\" for i in range(num_variables)]),\n",
    "    )\n",
    "\n",
    "\n",
    "# Do matrix multiplication to tranform data to PCA axis:\n",
    "pca_data = np.dot(data, evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(ncols=2)\n",
    "fig.tight_layout(pad=2)  # add some space\n",
    "\n",
    "ax1.plot(pca_data[:, 0], pca_data[:, 1], \"x\")\n",
    "ax1.set_xlabel(\"PC1\")\n",
    "ax1.set_ylabel(\"PC2\")\n",
    "ax1.set_title(\"First two princ. components\")\n",
    "\n",
    "ax2.plot(pca_data[:, 2], pca_data[:, 3], \"x\")\n",
    "ax2.set_xlabel(\"PC3\")\n",
    "ax2.set_ylabel(\"PC4\")\n",
    "ax2.set_title(\"Next two princ. components\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "### Put it all together in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(t_data, normalise=True, sub_mean=False):\n",
    "    \"\"\"Performs PCA. Returns transformed data, contributions, princ. components (e-vectors), sum of eigenvalues.\n",
    "    If normalise=True (default), will used normalised covariance.\n",
    "    If sub_mean=True, will subtract mean from each collumn before PCA.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import linalg as la\n",
    "\n",
    "    # Calculate covariance matrrix\n",
    "    cov = (\n",
    "        np.corrcoef(t_data, rowvar=False) if normalise else np.cov(t_data, rowvar=False)\n",
    "    )\n",
    "\n",
    "    # Gets the eigenvalues + eigenvectors\n",
    "    evals, evecs = la.eigh(cov)\n",
    "\n",
    "    # Sort eigen values/vectors\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "\n",
    "    # Contributions to variance (just e-vals as fraction of total):\n",
    "    contributions = evals / np.sum(evals)\n",
    "\n",
    "    # Optionally, subtract mean before doing PCA\n",
    "    mean = np.mean(t_data, axis=0) if sub_mean else 0\n",
    "\n",
    "    # PCA transform:\n",
    "    pca_data = np.dot(t_data - mean, evecs)\n",
    "\n",
    "    return pca_data, contributions, evecs, np.sum(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_labels = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "\n",
    "pca_data, contibutions, _, _ = pca(data, normalise=True, sub_mean=False)\n",
    "\n",
    "print(\"\\nComponent contributions:\")\n",
    "print(contributions)\n",
    "\n",
    "print(\"\\nCumulative component contributions:\")\n",
    "print(np.cumsum(contributions))\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(ncols=2)\n",
    "\n",
    "for i in range(1, num_variables):\n",
    "    ax1.plot(pca_data[:, 0], pca_data[:, i], \".\", label=pc_labels[i])\n",
    "ax1.set_xlabel(pc_labels[0])\n",
    "ax1.legend(loc=\"lower left\")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    ax2.plot(pca_data[:, 0], pca_data[:, i], \".\", label=pc_labels[i])\n",
    "ax2.set_xlabel(pc_labels[0])\n",
    "ax2.legend(loc=\"lower left\")\n",
    "\n",
    "plt.suptitle(\"Each variable as a function of p1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = corner_plot(pca_data, pc_labels, \"Corner plot (PCA components)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also use sklearn package (scikit-learn)\n",
    "\n",
    "* sk_PCA = sklearn.decomposition.PCA() \n",
    "* sk_PCA.fit_transform(data)\n",
    "* a) it doesn't normalise the covariance by default\n",
    "    * we can achieve the same result by scaling the data first ourselves\n",
    "    * scaled_data = StandardScaler().fit_transform(data)\n",
    "    * (or just manually divide by the variances)\n",
    "* b) it subtracts the mean by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaled_data = StandardScaler(with_mean=False).fit_transform(data)\n",
    "# scaled_data = data\n",
    "\n",
    "sk_pca = decomposition.PCA()\n",
    "sk_pca_data = sk_pca.fit_transform(scaled_data)\n",
    "pc_labels = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "\n",
    "print(\"sklearn: Explained variance fraction:\")\n",
    "print(sk_pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "for i in range(1, num_variables):\n",
    "    plt.plot(sk_pca_data[:, 0], sk_pca_data[:, i], \".\", label=pc_labels[i])\n",
    "plt.xlabel(pc_labels[0])\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.title(\"Each variable as a function of p1\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = corner_plot(sk_pca_data, pc_labels, \"Corner plot (PCA components)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "### Application to simplest case: 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xy = np.genfromtxt(\"data_xy.csv\", delimiter=\",\")\n",
    "\n",
    "plt.plot(data_xy[:, 0], data_xy[:, 1], \"x\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "# plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data_xy, p_contribs, p_components, evalsum = pca(\n",
    "    data_xy, normalise=False, sub_mean=False\n",
    ")\n",
    "\n",
    "print(p_contribs)\n",
    "\n",
    "plt.plot(pc_data_xy[:, 0], pc_data_xy[:, 1], \"x\")\n",
    "plt.xlabel(f\"pc1 = {p_components[0,0]:.2f}x + {p_components[1,0]:.2f}y\")\n",
    "plt.ylabel(f\"pc2 = {p_components[0,1]:.2f}x + {p_components[1,1]:.2f}y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_xy[:, 0], data_xy[:, 1], \"x\")\n",
    "\n",
    "means = np.mean(data_xy, axis=0)\n",
    "v1 = np.array([means, means + np.sqrt(evalsum * p_contribs[0]) * p_components[:, 0]])\n",
    "v2 = np.array([means, means + np.sqrt(evalsum * p_contribs[1]) * p_components[:, 1]])\n",
    "\n",
    "\n",
    "plt.plot(means[0], means[1], \"ro\", label=\"mean\", markersize=6)\n",
    "plt.plot(v1[:, 0], v1[:, 1], \"k-\", label=\"PC 1\", linewidth=3)\n",
    "plt.plot(v2[:, 0], v2[:, 1], \"k--\", label=\"PC 2\", linewidth=3)\n",
    "\n",
    "# plt.gca().set_aspect('equal')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Showing eigenvectors (scaled by sqrt[eval])\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A highly-redundant 4D data set:\n",
    "* Demonstrates deminsion reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wxyz = np.genfromtxt(\"data_wxyz.csv\", delimiter=\",\")\n",
    "labels = [\"w\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.set_box_aspect(aspect=None, zoom=0.83)\n",
    "ax.stem(\n",
    "    data_wxyz[:, 0][0::5], data_wxyz[:, 1][0::5], data_wxyz[:, 3][0::5], basefmt=\" \"\n",
    ")\n",
    "ax.set_xlabel(\"w\")\n",
    "ax.set_ylabel(\"x\")\n",
    "ax.set_zlabel(\"z\")\n",
    "ax.set_title(\"W,X,Z (no y), every 5th\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = corner_plot(data_wxyz, labels, \"WXYZ Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data_wxyz, p_contribs, p_components, _ = pca(data_wxyz, False)\n",
    "\n",
    "\n",
    "print(p_contribs)\n",
    "\n",
    "# print(p_components)\n",
    "\n",
    "pc_data_wxyz *= -1\n",
    "p_components *= -1\n",
    "\n",
    "plt.plot(pc_data_wxyz[:, 0], pc_data_wxyz[:, 1], \"x\")\n",
    "plt.xlabel(\n",
    "    f\"{p_components[0,0]:.2f}w + {p_components[1,0]:.2f}x + {p_components[2,0]:.2f}y + {p_components[3,0]:.2f}z\"\n",
    ")\n",
    "plt.ylabel(\n",
    "    f\"{p_components[0,1]:.2f}w + {p_components[1,1]:.2f}x + {p_components[2,1]:.2f}y + {p_components[3,1]:.2f}z\"\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = corner_plot(pc_data_wxyz, [\"p1\", \"p2\", \"p3\", \"p4\"], \"4D - PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse transform: dimensionality reduction\n",
    "\n",
    "* Inverse transform: simply invert the eigenvector matrix\n",
    "* Dimensionality reduction: discard all but lowest few PCAs\n",
    "* Inverse is slightly more tricky to code ourselves\n",
    "* Simple with library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_keep = 2\n",
    "sk_pca = decomposition.PCA(n_components=n_keep)\n",
    "# simply keeps only first 2 components (uses 2 eigenvalues)\n",
    "\n",
    "sk_pca_data = sk_pca.fit_transform(data_wxyz)\n",
    "\n",
    "print(sk_pca.explained_variance_ratio_)\n",
    "print(sk_pca.n_components_)\n",
    "print(sk_pca.components_)\n",
    "\n",
    "fig, axs = corner_plot(sk_pca_data, pc_labels[:n_keep], \"PCA Data\")\n",
    "\n",
    "\n",
    "data2 = sk_pca.inverse_transform(sk_pca_data)\n",
    "\n",
    "fig, axs = corner_plot(data2, labels, \"WXYZ Data, after D reduction\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
