{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy import linalg as LA\n",
    "\n",
    "data = np.genfromtxt(\"assp.csv\", delimiter=\",\")\n",
    "# Just give each column a random name\n",
    "collumns = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"]\n",
    "print(collumns)\n",
    "\n",
    "num_variables = len(collumns)\n",
    "\n",
    "assert num_variables == len(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine data\n",
    "* As usual, step 1 is to apply uni/bi-variate methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, num_variables):\n",
    "    plt.plot(data[:, 0], data[:, i], \".\", label=collumns[i])\n",
    "plt.xlabel(collumns[0])\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.title(\"Each variable as a function of a\")\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(ncols=2, width_ratios=[2, 1])\n",
    "ax1.violinplot(data[:, 0:4], showmeans=True)\n",
    "ax1.set_xticks([1, 2, 3, 4], collumns[:4])\n",
    "\n",
    "ax2.violinplot(data[:, 4:], showmeans=True)\n",
    "ax2.set_xticks([1, 2], collumns[4:])\n",
    "\n",
    "plt.suptitle(\"Violin plot, showing means\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corner_plot(data, labels, title):\n",
    "    num_variables = len(labels)\n",
    "    fig, axs = plt.subplots(nrows=num_variables, ncols=num_variables, figsize=(7, 7))\n",
    "    for i in range(num_variables):\n",
    "        for j in range(num_variables):\n",
    "\n",
    "            # Only plot unique lower triangle\n",
    "            if j > i:\n",
    "                axs[i, j].set_visible(False)\n",
    "                continue\n",
    "\n",
    "            # Plot the data\n",
    "\n",
    "            # Scatter plot for when x is not y\n",
    "            if i != j:\n",
    "                axs[i, j].plot(data[:, j], data[:, i], \".\")\n",
    "\n",
    "            # When x=y the plot would just be a straight line\n",
    "            # It's common to plot histgoram (or box plot, etc)\n",
    "            else:\n",
    "                axs[i, j].hist(data[:, i], density=True, alpha=0.4)\n",
    "                x = np.linspace(min(data[:, i]), max(data[:, i]), 100)\n",
    "                kde = stats.gaussian_kde(data[:, i])\n",
    "                axs[i, j].plot(x, kde(x), \"b\")\n",
    "                axs[i, j].fill_between(x, kde(x), alpha=0.6)\n",
    "                axs[i, j].set_yticks([])\n",
    "\n",
    "            # Add title (mean and sem) above diagonal elements:\n",
    "            if i == j:\n",
    "                mean = np.mean(data[:, i])\n",
    "                sem = stats.sem(data[:, i], ddof=1)\n",
    "                axs[i, j].set_title(\n",
    "                    f\"{labels[j]}\\n{mean:.1f}$\\\\pm${sem:.1f}\", fontsize=11\n",
    "                )\n",
    "\n",
    "            # Add x labels only to last row\n",
    "            if i == num_variables - 1:\n",
    "                axs[i, j].set_xlabel(labels[j], fontsize=12)\n",
    "            else:\n",
    "                axs[i, j].set_xticks([])\n",
    "\n",
    "            # Add y labels only to first column\n",
    "            if j == 0:\n",
    "                axs[i, j].set_ylabel(labels[i], fontsize=12)\n",
    "            else:\n",
    "                axs[i, j].set_yticks([])\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    fig.align_ylabels(axs[:, 0])\n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "fig, axs = corner_plot(data, collumns, \"Corner plot (raw data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives the mean of each column:\n",
    "column_means = np.mean(data, axis=0)\n",
    "\n",
    "# Change number of digits printed\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "print(column_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component analysis\n",
    "\n",
    "### 1. Calculate covariance matrix\n",
    "\n",
    "**Reminder**\n",
    "\n",
    "Variance (sample variance):\n",
    "\n",
    "$$\n",
    "  \\sigma^2(X) = \\langle{(X-\\bar X)^2}\\rangle = \\sum_i \\frac{(X-\\bar X)^2}{N-1}\n",
    "$$\n",
    "\n",
    "**Co**-variance\n",
    "\n",
    "$$\n",
    "  {\\rm cov}(X,Y) = \\langle{(X-\\bar X)(Y-\\bar Y)}\\rangle = \\sum_i \\frac{(X-\\bar X)(Y-\\bar Y)}{N-1}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "  {\\rm cov}(X,X) = \\sigma^2(X)\n",
    "$$\n",
    "\n",
    "\n",
    "**Normalised** covariance (also called correlation coefficent)\n",
    "\n",
    "$$\n",
    "  {\\rm ncov}(X,Y) = \\frac{{\\rm cov}(X,Y)}{\\sqrt{\\sigma^2(X)\\,\\sigma^2(Y)}}\n",
    "$$\n",
    "\n",
    "Note that the normalised covariance is related to the autocorrelation function:\n",
    "$$\n",
    "  {\\rm ncov}\\Big(X(t),\\,X(t+\\Delta t)\\Big) = {\\rm ACF}(\\Delta t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance matrix\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.cov.html\n",
    "# If rowvar is True (default), then each row represents a variable\n",
    "\n",
    "cov = np.cov(data, rowvar=False)\n",
    "\n",
    "print(\"Covariance matrix:\")\n",
    "print(cov)\n",
    "\n",
    "\n",
    "column_variances = np.var(data, axis=0, ddof=1)\n",
    "print(\"\\nVariances of each collumn (a,b,c..):\")\n",
    "print(column_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"normalised\" covariance (or correlation coeficient)\n",
    "# corrcoef = R_ij := C_ij / Sqrt( C_ii*C_jj )\n",
    "ncov = np.corrcoef(data, rowvar=False)\n",
    "\n",
    "print(\"\\nNormalised Covariance matrix:\")\n",
    "print(ncov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Find principal components: eigenvalue problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use eigh (instead of eig) since covariance is symmetric\n",
    "evals, evecs = LA.eigh(ncov)\n",
    "\n",
    "# Unsorted e.vals:\n",
    "print(evals)\n",
    "\n",
    "\n",
    "# The eigenvalues are not guarenteed to be in any specific order\n",
    "# It's nice to have them sorted\n",
    "# Must use argsort since we need to sort eigenvectors in same way\n",
    "idx = np.argsort(evals)[::-1]\n",
    "print(\"order:\", idx)\n",
    "\n",
    "# Sort the e.vals and e.vectors:\n",
    "evals = evals[idx]\n",
    "evecs = evecs[:, idx]\n",
    "\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(evals)\n",
    "\n",
    "# Eigenvalues are the relative contibutions\n",
    "contributions = evals / np.sum(evals)\n",
    "\n",
    "print(\"\\nComponent contributions:\")\n",
    "print(contributions)\n",
    "\n",
    "print(\"\\nCumulative component contributions:\")\n",
    "print(np.cumsum(contributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvectors:\")\n",
    "print(evecs)\n",
    "\n",
    "print(\"\\nPrincipal components:\")\n",
    "for n in range(num_variables):\n",
    "    print(\n",
    "        f\"PCA({n}): \",\n",
    "        \"\".join([f\" {evecs[i,n]:+.2f}*{collumns[i]}\" for i in range(num_variables)]),\n",
    "    )\n",
    "\n",
    "\n",
    "# Do matrix multiplication to tranform data to PCA axis:\n",
    "pca_data = np.dot(data, evecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca_data[:, 0], pca_data[:, 1], \"x\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"First two princ. components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(t_data, normalise=True, sub_mean=False):\n",
    "    \"\"\"Performs PCA. Returns transformed data, and contributions.\n",
    "    If normalise=True (default), will used normalised covariance.\n",
    "    If sub_mean=True, will subtract mean from each collumn before PCA.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import linalg as la\n",
    "\n",
    "    # t_data -= np.mean(t_data, axis=0)\n",
    "    # t_data /= np.std(t_data, axis=0, ddof=1)\n",
    "\n",
    "    cov = (\n",
    "        np.corrcoef(t_data, rowvar=False) if normalise else np.cov(t_data, rowvar=False)\n",
    "    )\n",
    "\n",
    "    evals, evecs = la.eigh(cov)\n",
    "\n",
    "    # Sort eigen values/vectors\n",
    "    idx = np.argsort(evals)[::-1]\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "\n",
    "    # Contributions to variance:\n",
    "    contributions = evals / np.sum(evals)\n",
    "\n",
    "    mean = np.mean(t_data, axis=0) if sub_mean else 0\n",
    "\n",
    "    # PCA transform:\n",
    "    pca_data = np.dot(t_data - mean, evecs)\n",
    "\n",
    "    return pca_data, contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_labels = [\"p1\", \"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "\n",
    "pca_data, contibutions = pca(data, normalise=True, sub_mean=False)\n",
    "\n",
    "print(\"\\nComponent contributions:\")\n",
    "print(contributions)\n",
    "\n",
    "print(\"\\nCumulative component contributions:\")\n",
    "print(np.cumsum(contributions))\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(ncols=2)\n",
    "\n",
    "for i in range(1, num_variables):\n",
    "    ax1.plot(pca_data[:, 0], pca_data[:, i], \".\", label=pc_labels[i])\n",
    "ax1.set_xlabel(pc_labels[0])\n",
    "ax1.legend(loc=\"lower left\")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    ax2.plot(pca_data[:, 0], pca_data[:, i], \".\", label=pc_labels[i])\n",
    "ax2.set_xlabel(pc_labels[0])\n",
    "ax2.legend(loc=\"lower left\")\n",
    "\n",
    "plt.suptitle(\"Each variable as a function of p1\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = corner_plot(pca_data, pc_labels, \"Corner plot (PCA components)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaled_data = StandardScaler(with_mean=False).fit_transform(data)\n",
    "\n",
    "sk_pca = decomposition.PCA()\n",
    "sk_pca_data = sk_pca.fit_transform(scaled_data)\n",
    "\n",
    "# print(\"sklearn: Explained variance fraction:\")\n",
    "# print(sk_pca.components_)\n",
    "# print(evecs)\n",
    "\n",
    "\n",
    "print(\"sklearn: Explained variance fraction:\")\n",
    "print(sk_pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "for i in range(1, num_variables):\n",
    "    plt.plot(pca_data[:, 0], pca_data[:, i], \".\", label=pc_labels[i])\n",
    "plt.xlabel(pc_labels[0])\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plt.title(\"Each variable as a function of p1\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = corner_plot(sk_pca_data, pc_labels, \"Corner plot (PCA components)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
